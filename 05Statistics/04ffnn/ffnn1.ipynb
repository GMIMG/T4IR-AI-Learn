{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. custom FFNN\n",
    "#### 1) 활성화함수가 있는 FFNN\n",
    "활성화 함수가 있는 FFNN를 구현해 보자.\n",
    "$$\\begin{aligned}\n",
    "s &= x \\cdot w + b \\\\\n",
    "\\hat y &= f(s) \\\\\n",
    "e &= (\\hat y - y)^2 \\\\ \n",
    "& \\\\\n",
    "\\cfrac {\\partial e}{\\partial w} &= \\cfrac{\\partial e}{\\partial \\hat y} \\cfrac{\\partial \\hat y}{\\partial s} \\cfrac{\\partial s}{\\partial w} \\\\\n",
    "&= x^T \\cdot 2(\\hat y - y)f^{'}(s)\\\\\n",
    "(p, h=1) &= (b, p)^T \\cdot [(B, 1) \\times (B, 1)]\n",
    "\\end{aligned}$$\n",
    "\n",
    "참고로:\n",
    "$$\\begin{aligned}\n",
    "f(x) &= \\tanh(x) \\\\\n",
    "f^{'}(x) &= 1 - \\tanh^2(x)\n",
    "\\end{aligned}$$\n",
    "\n",
    "이를 위해 propagate_forward(self, x)함수를 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.random.rand(1024, 1)\n",
    "y_train = x_train * 0.1 - 0.05\n",
    "\n",
    "x_val = np.random.rand(32).reshape(-1,1)\n",
    "y_val = x_val * 0.1 - 0.05\n",
    "\n",
    "x_test = np.arange(0, 10, 2).reshape(-1,1)*.1\n",
    "y_test = x_test * 0.1 - 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        # initial weights: w는 .5, b는 .0\n",
    "        self.w = np.array([0.5]).reshape(-1, 1)\n",
    "        self.b = np.array([0.])\n",
    "        \n",
    "    def calc_sum(self, x, w, b):\n",
    "        return np.dot(x, w) + b\n",
    "    \n",
    "    # feed forward network\n",
    "    def predict(self, x):\n",
    "        return np.tanh(self.calc_sum(x, self.w, self.b))\n",
    "    \n",
    "    # train for one batch. x 자체가 batch\n",
    "    def train_on_batch(self, x, y, istrain=True):\n",
    "        N = x.shape[0]\n",
    "        s = self.calc_sum(x, self.w, self.b)\n",
    "        Y = self.predict(x)\n",
    "        loss = 1/2 * np.sum(Y - y)**2\n",
    "        if istrain:\n",
    "            #dw = 1 / N * x.T.dot((self.predict(x) - y) * (1 - self.predict(x)**2))\n",
    "            dw = 1 / N * x.T.dot((Y - y) * (1 - np.tanh(s)**2))\n",
    "            db = 1 / N * np.sum((Y - y) * (1 - np.tanh(s)**2))\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, x, y, batch_size=16, epochs=100, val_data=None):\n",
    "        Losses = {}\n",
    "        Losses[\"train_loss\"] = []\n",
    "        if val_data is not None:\n",
    "            Losses[\"val_loss\"] = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            Loss = 0\n",
    "            \n",
    "            N = x.shape[0]\n",
    "            \n",
    "            for j in range(0, N, batch_size):\n",
    "                x_batch = x[j:j + batch_size]\n",
    "                y_batch = y[j:j + batch_size]\n",
    "                n = x_batch.shape[0]\n",
    "                Loss += (self.train_on_batch(x_batch, y_batch, istrain=False) / n)\n",
    "            print(\"Train Loss at Epoch %d is %.8f\" %(i, Loss))\n",
    "            Losses[\"train_loss\"].append(Loss)\n",
    "            if val_data is not None:\n",
    "                val_N = val_data[0].shape[0]\n",
    "                val_loss = self.train_on_batch(*val_data) / val_N\n",
    "                print(\"Val Loss at Epoch %d is %.8f\" %(i, val_loss))\n",
    "                Losses[\"val_loss\"].append(val_loss)\n",
    "                \n",
    "        return Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at Epoch 0 is 30.32059871\n",
      "Val Loss at Epoch 0 is 0.77887092\n",
      "Train Loss at Epoch 1 is 29.74068278\n",
      "Val Loss at Epoch 1 is 0.76269730\n",
      "Train Loss at Epoch 2 is 29.17046580\n",
      "Val Loss at Epoch 2 is 0.74680983\n",
      "Train Loss at Epoch 3 is 28.60983914\n",
      "Val Loss at Epoch 3 is 0.73120506\n",
      "Train Loss at Epoch 4 is 28.05869344\n",
      "Val Loss at Epoch 4 is 0.71587949\n",
      "Train Loss at Epoch 5 is 27.51691875\n",
      "Val Loss at Epoch 5 is 0.70082965\n",
      "Train Loss at Epoch 6 is 26.98440457\n",
      "Val Loss at Epoch 6 is 0.68605203\n",
      "Train Loss at Epoch 7 is 26.46103993\n",
      "Val Loss at Epoch 7 is 0.67154312\n",
      "Train Loss at Epoch 8 is 25.94671351\n",
      "Val Loss at Epoch 8 is 0.65729942\n",
      "Train Loss at Epoch 9 is 25.44131366\n",
      "Val Loss at Epoch 9 is 0.64331740\n",
      "Train Loss at Epoch 10 is 24.94472849\n",
      "Val Loss at Epoch 10 is 0.62959357\n",
      "Train Loss at Epoch 11 is 24.45684594\n",
      "Val Loss at Epoch 11 is 0.61612440\n",
      "Train Loss at Epoch 12 is 23.97755387\n",
      "Val Loss at Epoch 12 is 0.60290640\n",
      "Train Loss at Epoch 13 is 23.50674007\n",
      "Val Loss at Epoch 13 is 0.58993605\n",
      "Train Loss at Epoch 14 is 23.04429239\n",
      "Val Loss at Epoch 14 is 0.57720987\n",
      "Train Loss at Epoch 15 is 22.59009873\n",
      "Val Loss at Epoch 15 is 0.56472436\n",
      "Train Loss at Epoch 16 is 22.14404718\n",
      "Val Loss at Epoch 16 is 0.55247606\n",
      "Train Loss at Epoch 17 is 21.70602599\n",
      "Val Loss at Epoch 17 is 0.54046150\n",
      "Train Loss at Epoch 18 is 21.27592370\n",
      "Val Loss at Epoch 18 is 0.52867724\n",
      "Train Loss at Epoch 19 is 20.85362915\n",
      "Val Loss at Epoch 19 is 0.51711983\n",
      "Train Loss at Epoch 20 is 20.43903154\n",
      "Val Loss at Epoch 20 is 0.50578586\n",
      "Train Loss at Epoch 21 is 20.03202046\n",
      "Val Loss at Epoch 21 is 0.49467194\n",
      "Train Loss at Epoch 22 is 19.63248601\n",
      "Val Loss at Epoch 22 is 0.48377468\n",
      "Train Loss at Epoch 23 is 19.24031873\n",
      "Val Loss at Epoch 23 is 0.47309072\n",
      "Train Loss at Epoch 24 is 18.85540974\n",
      "Val Loss at Epoch 24 is 0.46261673\n",
      "Train Loss at Epoch 25 is 18.47765074\n",
      "Val Loss at Epoch 25 is 0.45234939\n",
      "Train Loss at Epoch 26 is 18.10693405\n",
      "Val Loss at Epoch 26 is 0.44228540\n",
      "Train Loss at Epoch 27 is 17.74315266\n",
      "Val Loss at Epoch 27 is 0.43242150\n",
      "Train Loss at Epoch 28 is 17.38620024\n",
      "Val Loss at Epoch 28 is 0.42275445\n",
      "Train Loss at Epoch 29 is 17.03597120\n",
      "Val Loss at Epoch 29 is 0.41328104\n",
      "Train Loss at Epoch 30 is 16.69236073\n",
      "Val Loss at Epoch 30 is 0.40399807\n",
      "Train Loss at Epoch 31 is 16.35526477\n",
      "Val Loss at Epoch 31 is 0.39490238\n",
      "Train Loss at Epoch 32 is 16.02458012\n",
      "Val Loss at Epoch 32 is 0.38599085\n",
      "Train Loss at Epoch 33 is 15.70020442\n",
      "Val Loss at Epoch 33 is 0.37726037\n",
      "Train Loss at Epoch 34 is 15.38203616\n",
      "Val Loss at Epoch 34 is 0.36870787\n",
      "Train Loss at Epoch 35 is 15.06997475\n",
      "Val Loss at Epoch 35 is 0.36033031\n",
      "Train Loss at Epoch 36 is 14.76392052\n",
      "Val Loss at Epoch 36 is 0.35212469\n",
      "Train Loss at Epoch 37 is 14.46377473\n",
      "Val Loss at Epoch 37 is 0.34408801\n",
      "Train Loss at Epoch 38 is 14.16943958\n",
      "Val Loss at Epoch 38 is 0.33621734\n",
      "Train Loss at Epoch 39 is 13.88081829\n",
      "Val Loss at Epoch 39 is 0.32850977\n",
      "Train Loss at Epoch 40 is 13.59781504\n",
      "Val Loss at Epoch 40 is 0.32096240\n",
      "Train Loss at Epoch 41 is 13.32033503\n",
      "Val Loss at Epoch 41 is 0.31357241\n",
      "Train Loss at Epoch 42 is 13.04828447\n",
      "Val Loss at Epoch 42 is 0.30633697\n",
      "Train Loss at Epoch 43 is 12.78157062\n",
      "Val Loss at Epoch 43 is 0.29925330\n",
      "Train Loss at Epoch 44 is 12.52010177\n",
      "Val Loss at Epoch 44 is 0.29231866\n",
      "Train Loss at Epoch 45 is 12.26378728\n",
      "Val Loss at Epoch 45 is 0.28553035\n",
      "Train Loss at Epoch 46 is 12.01253757\n",
      "Val Loss at Epoch 46 is 0.27888568\n",
      "Train Loss at Epoch 47 is 11.76626412\n",
      "Val Loss at Epoch 47 is 0.27238200\n",
      "Train Loss at Epoch 48 is 11.52487952\n",
      "Val Loss at Epoch 48 is 0.26601673\n",
      "Train Loss at Epoch 49 is 11.28829741\n",
      "Val Loss at Epoch 49 is 0.25978728\n",
      "Train Loss at Epoch 50 is 11.05643254\n",
      "Val Loss at Epoch 50 is 0.25369111\n",
      "Train Loss at Epoch 51 is 10.82920076\n",
      "Val Loss at Epoch 51 is 0.24772574\n",
      "Train Loss at Epoch 52 is 10.60651899\n",
      "Val Loss at Epoch 52 is 0.24188868\n",
      "Train Loss at Epoch 53 is 10.38830528\n",
      "Val Loss at Epoch 53 is 0.23617751\n",
      "Train Loss at Epoch 54 is 10.17447877\n",
      "Val Loss at Epoch 54 is 0.23058982\n",
      "Train Loss at Epoch 55 is 9.96495970\n",
      "Val Loss at Epoch 55 is 0.22512327\n",
      "Train Loss at Epoch 56 is 9.75966941\n",
      "Val Loss at Epoch 56 is 0.21977552\n",
      "Train Loss at Epoch 57 is 9.55853034\n",
      "Val Loss at Epoch 57 is 0.21454428\n",
      "Train Loss at Epoch 58 is 9.36146603\n",
      "Val Loss at Epoch 58 is 0.20942728\n",
      "Train Loss at Epoch 59 is 9.16840111\n",
      "Val Loss at Epoch 59 is 0.20442232\n",
      "Train Loss at Epoch 60 is 8.97926133\n",
      "Val Loss at Epoch 60 is 0.19952719\n",
      "Train Loss at Epoch 61 is 8.79397350\n",
      "Val Loss at Epoch 61 is 0.19473974\n",
      "Train Loss at Epoch 62 is 8.61246552\n",
      "Val Loss at Epoch 62 is 0.19005785\n",
      "Train Loss at Epoch 63 is 8.43466638\n",
      "Val Loss at Epoch 63 is 0.18547944\n",
      "Train Loss at Epoch 64 is 8.26050615\n",
      "Val Loss at Epoch 64 is 0.18100244\n",
      "Train Loss at Epoch 65 is 8.08991596\n",
      "Val Loss at Epoch 65 is 0.17662484\n",
      "Train Loss at Epoch 66 is 7.92282801\n",
      "Val Loss at Epoch 66 is 0.17234465\n",
      "Train Loss at Epoch 67 is 7.75917555\n",
      "Val Loss at Epoch 67 is 0.16815991\n",
      "Train Loss at Epoch 68 is 7.59889288\n",
      "Val Loss at Epoch 68 is 0.16406871\n",
      "Train Loss at Epoch 69 is 7.44191536\n",
      "Val Loss at Epoch 69 is 0.16006914\n",
      "Train Loss at Epoch 70 is 7.28817936\n",
      "Val Loss at Epoch 70 is 0.15615935\n",
      "Train Loss at Epoch 71 is 7.13762231\n",
      "Val Loss at Epoch 71 is 0.15233751\n",
      "Train Loss at Epoch 72 is 6.99018262\n",
      "Val Loss at Epoch 72 is 0.14860183\n",
      "Train Loss at Epoch 73 is 6.84579974\n",
      "Val Loss at Epoch 73 is 0.14495054\n",
      "Train Loss at Epoch 74 is 6.70441411\n",
      "Val Loss at Epoch 74 is 0.14138191\n",
      "Train Loss at Epoch 75 is 6.56596716\n",
      "Val Loss at Epoch 75 is 0.13789423\n",
      "Train Loss at Epoch 76 is 6.43040130\n",
      "Val Loss at Epoch 76 is 0.13448583\n",
      "Train Loss at Epoch 77 is 6.29765992\n",
      "Val Loss at Epoch 77 is 0.13115506\n",
      "Train Loss at Epoch 78 is 6.16768738\n",
      "Val Loss at Epoch 78 is 0.12790032\n",
      "Train Loss at Epoch 79 is 6.04042896\n",
      "Val Loss at Epoch 79 is 0.12472000\n",
      "Train Loss at Epoch 80 is 5.91583092\n",
      "Val Loss at Epoch 80 is 0.12161256\n",
      "Train Loss at Epoch 81 is 5.79384043\n",
      "Val Loss at Epoch 81 is 0.11857646\n",
      "Train Loss at Epoch 82 is 5.67440558\n",
      "Val Loss at Epoch 82 is 0.11561021\n",
      "Train Loss at Epoch 83 is 5.55747538\n",
      "Val Loss at Epoch 83 is 0.11271232\n",
      "Train Loss at Epoch 84 is 5.44299975\n",
      "Val Loss at Epoch 84 is 0.10988136\n",
      "Train Loss at Epoch 85 is 5.33092946\n",
      "Val Loss at Epoch 85 is 0.10711591\n",
      "Train Loss at Epoch 86 is 5.22121620\n",
      "Val Loss at Epoch 86 is 0.10441456\n",
      "Train Loss at Epoch 87 is 5.11381249\n",
      "Val Loss at Epoch 87 is 0.10177596\n",
      "Train Loss at Epoch 88 is 5.00867174\n",
      "Val Loss at Epoch 88 is 0.09919876\n",
      "Train Loss at Epoch 89 is 4.90574819\n",
      "Val Loss at Epoch 89 is 0.09668165\n",
      "Train Loss at Epoch 90 is 4.80499689\n",
      "Val Loss at Epoch 90 is 0.09422334\n",
      "Train Loss at Epoch 91 is 4.70637374\n",
      "Val Loss at Epoch 91 is 0.09182255\n",
      "Train Loss at Epoch 92 is 4.60983546\n",
      "Val Loss at Epoch 92 is 0.08947805\n",
      "Train Loss at Epoch 93 is 4.51533952\n",
      "Val Loss at Epoch 93 is 0.08718862\n",
      "Train Loss at Epoch 94 is 4.42284423\n",
      "Val Loss at Epoch 94 is 0.08495306\n",
      "Train Loss at Epoch 95 is 4.33230865\n",
      "Val Loss at Epoch 95 is 0.08277021\n",
      "Train Loss at Epoch 96 is 4.24369261\n",
      "Val Loss at Epoch 96 is 0.08063892\n",
      "Train Loss at Epoch 97 is 4.15695669\n",
      "Val Loss at Epoch 97 is 0.07855806\n",
      "Train Loss at Epoch 98 is 4.07206222\n",
      "Val Loss at Epoch 98 is 0.07652652\n",
      "Train Loss at Epoch 99 is 3.98897125\n",
      "Val Loss at Epoch 99 is 0.07454323\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, val_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b9de7845c0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# loss의 결과를 시각화하세요.\n",
    "pd.DataFrame(hist).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.320599</td>\n",
       "      <td>0.778871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.740683</td>\n",
       "      <td>0.762697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.170466</td>\n",
       "      <td>0.746810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.609839</td>\n",
       "      <td>0.731205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.058693</td>\n",
       "      <td>0.715879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27.516919</td>\n",
       "      <td>0.700830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26.984405</td>\n",
       "      <td>0.686052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.461040</td>\n",
       "      <td>0.671543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25.946714</td>\n",
       "      <td>0.657299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25.441314</td>\n",
       "      <td>0.643317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24.944728</td>\n",
       "      <td>0.629594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24.456846</td>\n",
       "      <td>0.616124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23.977554</td>\n",
       "      <td>0.602906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23.506740</td>\n",
       "      <td>0.589936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>23.044292</td>\n",
       "      <td>0.577210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22.590099</td>\n",
       "      <td>0.564724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22.144047</td>\n",
       "      <td>0.552476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21.706026</td>\n",
       "      <td>0.540462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21.275924</td>\n",
       "      <td>0.528677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.853629</td>\n",
       "      <td>0.517120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.439032</td>\n",
       "      <td>0.505786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20.032020</td>\n",
       "      <td>0.494672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19.632486</td>\n",
       "      <td>0.483775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19.240319</td>\n",
       "      <td>0.473091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>18.855410</td>\n",
       "      <td>0.462617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18.477651</td>\n",
       "      <td>0.452349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.106934</td>\n",
       "      <td>0.442285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17.743153</td>\n",
       "      <td>0.432422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>17.386200</td>\n",
       "      <td>0.422754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.035971</td>\n",
       "      <td>0.413281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>7.288179</td>\n",
       "      <td>0.156159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>7.137622</td>\n",
       "      <td>0.152338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.990183</td>\n",
       "      <td>0.148602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.845800</td>\n",
       "      <td>0.144951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.704414</td>\n",
       "      <td>0.141382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6.565967</td>\n",
       "      <td>0.137894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.430401</td>\n",
       "      <td>0.134486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6.297660</td>\n",
       "      <td>0.131155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>6.167687</td>\n",
       "      <td>0.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>6.040429</td>\n",
       "      <td>0.124720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5.915831</td>\n",
       "      <td>0.121613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5.793840</td>\n",
       "      <td>0.118576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5.674406</td>\n",
       "      <td>0.115610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>5.557475</td>\n",
       "      <td>0.112712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5.443000</td>\n",
       "      <td>0.109881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5.330929</td>\n",
       "      <td>0.107116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>5.221216</td>\n",
       "      <td>0.104415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>5.113812</td>\n",
       "      <td>0.101776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5.008672</td>\n",
       "      <td>0.099199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>4.905748</td>\n",
       "      <td>0.096682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>4.804997</td>\n",
       "      <td>0.094223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>4.706374</td>\n",
       "      <td>0.091823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>4.609835</td>\n",
       "      <td>0.089478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>4.515340</td>\n",
       "      <td>0.087189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4.422844</td>\n",
       "      <td>0.084953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4.332309</td>\n",
       "      <td>0.082770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4.243693</td>\n",
       "      <td>0.080639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4.156957</td>\n",
       "      <td>0.078558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4.072062</td>\n",
       "      <td>0.076527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.988971</td>\n",
       "      <td>0.074543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  val_loss\n",
       "0    30.320599  0.778871\n",
       "1    29.740683  0.762697\n",
       "2    29.170466  0.746810\n",
       "3    28.609839  0.731205\n",
       "4    28.058693  0.715879\n",
       "5    27.516919  0.700830\n",
       "6    26.984405  0.686052\n",
       "7    26.461040  0.671543\n",
       "8    25.946714  0.657299\n",
       "9    25.441314  0.643317\n",
       "10   24.944728  0.629594\n",
       "11   24.456846  0.616124\n",
       "12   23.977554  0.602906\n",
       "13   23.506740  0.589936\n",
       "14   23.044292  0.577210\n",
       "15   22.590099  0.564724\n",
       "16   22.144047  0.552476\n",
       "17   21.706026  0.540462\n",
       "18   21.275924  0.528677\n",
       "19   20.853629  0.517120\n",
       "20   20.439032  0.505786\n",
       "21   20.032020  0.494672\n",
       "22   19.632486  0.483775\n",
       "23   19.240319  0.473091\n",
       "24   18.855410  0.462617\n",
       "25   18.477651  0.452349\n",
       "26   18.106934  0.442285\n",
       "27   17.743153  0.432422\n",
       "28   17.386200  0.422754\n",
       "29   17.035971  0.413281\n",
       "..         ...       ...\n",
       "70    7.288179  0.156159\n",
       "71    7.137622  0.152338\n",
       "72    6.990183  0.148602\n",
       "73    6.845800  0.144951\n",
       "74    6.704414  0.141382\n",
       "75    6.565967  0.137894\n",
       "76    6.430401  0.134486\n",
       "77    6.297660  0.131155\n",
       "78    6.167687  0.127900\n",
       "79    6.040429  0.124720\n",
       "80    5.915831  0.121613\n",
       "81    5.793840  0.118576\n",
       "82    5.674406  0.115610\n",
       "83    5.557475  0.112712\n",
       "84    5.443000  0.109881\n",
       "85    5.330929  0.107116\n",
       "86    5.221216  0.104415\n",
       "87    5.113812  0.101776\n",
       "88    5.008672  0.099199\n",
       "89    4.905748  0.096682\n",
       "90    4.804997  0.094223\n",
       "91    4.706374  0.091823\n",
       "92    4.609835  0.089478\n",
       "93    4.515340  0.087189\n",
       "94    4.422844  0.084953\n",
       "95    4.332309  0.082770\n",
       "96    4.243693  0.080639\n",
       "97    4.156957  0.078558\n",
       "98    4.072062  0.076527\n",
       "99    3.988971  0.074543\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.121774</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039651</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.043011</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125088</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.205489</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred  target\n",
       "0 -0.121774   -0.05\n",
       "1 -0.039651   -0.03\n",
       "2  0.043011   -0.01\n",
       "3  0.125088    0.01\n",
       "4  0.205489    0.03"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'pred':model.predict(x_test).ravel(), 'target':y_test.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
